This video will give you an overview of the ETL lesson of the ETL & Data Pipelines module. In this module, you will perform two assignments, each with several exercises. In the first assignment, covered in this video, you will create an ETL pipeline using Python to get new data for the day from the OLTP database and load the Data Warehouse. In the second assignment, covered in the following video, you will create a data pipeline using Apache Airflow to feed the big data cluster. In this assignment, you will perform four exercises. But before proceeding with the assignment, you will need to perform these tasks: Prepare the lab environment by starting MySQL server, and downloading the files database from the given link. Further, you will also need to perform these tasks: Import the data in the JSON file to a database and a collection, and the data in the SQL file to the MySQL server. 
You will also verify your access to the cloud instance of the IBM Db2 server. The first exercise requires you to extract the data from the sales tables in MySQL into CSV format. In the second exercise you will complete the following tasks: transform the OLTP data to suit the data warehouse schema, read the OLTP data in the CSV format, add, edit, and drop columns based on the data warehouse schema, and then save the transformed data into a new CSV file. In the third exercise, you will perform tasks to load the transformed data into the data warehouse. You will need to perform the following tasks: read the transformed data in the CSV format, load the data into the data warehouse, and then, verify that the data is loaded properly. The final exercise requires you to perform the following task: Automate the extraction of daily incremental data and load yesterday's data into the data warehouse. Then, you will download the python script from the link provided, and use it as a template to write a python script that automatically loads yesterday's data from the production database into the data warehouse. 
After performing each task, take a screenshot of the command you used and its output, and name the screenshot. Good luck and letâ€™s get started! 